{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8e7588-6353-47af-8ae6-23578124d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13f7cc7-c1b0-4ea0-a9d5-3fb24422464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_filepath = \"/mnt/data1/datasets/hatespeech/latent_hatred/truncated/explanations-revised/train-explanations.jsonl\"\n",
    "\n",
    "misogynistic_filepath = \"/mnt/data1/datasets/memes/Misogynistic_MEME/annotations/explanation.jsonl\"\n",
    "misogynistic_features_dir = \"/mnt/data1/datasets/memes/cmtl-rag/mmhs/embeddings/clip-ViT-B-32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfa378d-f931-4930-9dbd-f7e8c06a778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_caption(filename, caption_dir):\n",
    "    caption_filepath = os.path.join(caption_dir, filename)\n",
    "    with open(caption_filepath) as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    return d['caption']\n",
    "\n",
    "\n",
    "def load_features(features_dir, filename):\n",
    "    # Iterate over all files in the folder\n",
    "    file_path = os.path.join(features_dir, filename)\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_majority_voting(lst):\n",
    "    from collections import Counter\n",
    "    count = Counter(lst)\n",
    "    most_common = count.most_common(1)\n",
    "    return most_common[0][0] if most_common else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5eaed20-45f2-4554-a8f2-b34b18528b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Targets: 326\n",
      "Number of Targets Mappings: 8\n",
      "Targets Mappings: {'political', 'religion', 'disability', 'organisations', 'race', 'others', 'nationality', 'sex'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"lh_target_mapping.json\") as f:\n",
    "    target_mapping = json.load(f)\n",
    "\n",
    "print(\"Number of Targets:\", len(target_mapping))\n",
    "print(\"Number of Targets Mappings:\", len(set(target_mapping.values())))\n",
    "print(\"Targets Mappings:\", set(target_mapping.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d609977-cb93-4348-b35d-eb86645f23a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{None: 0}\n",
      "{None: 0, 'political': 1, 'religion': 2, 'disability': 3, 'organisations': 4, 'race': 5, 'others': 6, 'nationality': 7, 'sex': 8}\n"
     ]
    }
   ],
   "source": [
    "target2label = {\n",
    "    None: 0\n",
    "}\n",
    "\n",
    "print(target2label)\n",
    "for idx, v in enumerate(set(target_mapping.values())):\n",
    "    target2label[v] = idx + 1\n",
    "\n",
    "print(target2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c23fbd6-50f3-423b-ba77-3345366c35d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'lh_916944251635752960',\n",
       " 'img': None,\n",
       " 'caption': None,\n",
       " 'text': ' i hate to break to you  but hey all you  \" white \" \" and \" \" black \" \" people ? you are the immigrant . have a great day . \"',\n",
       " 'label': 1,\n",
       " 'implicit_class': None,\n",
       " 'target': 'Whites and Blacks',\n",
       " 'target_category': 'race',\n",
       " 'rationale': '1. Targeted Group: The post targets all White and Black people by referring to them as \"you\" and \"hey all you\" without any consideration for individual differences. This lumping of people into categories based on their race is a broad generalization and perpetuates a harmful divide between races.\\n2. Derogatory Imagery or Language: Although not explicitly stated, the use of \"hey all you \\'white\\' \\'black\\' people\" implies that both white and black people are inferior to someone else. This is a form of divisive language that fuels racism and contributes to a hostile environment.\\n3. Impact on Bias, Stereotypes, Prejudice, Discrimination, or Inciting Harm: The post\\'s hateful undertones contribute to perpetuating harmful stereotypes about immigrants and racial biases. Immigrants are not a monolithic group and come from diverse backgrounds, but this post fails to acknowledge that. Instead, it reinforces a narrative that all individuals can be categorized and judged based on their race, and that they are inherently different and lesser than others.\\n\\nIn summary, the post\\'s hateful language harms individuals based on their race (white and black) and reinforces harmful stereotypes and biases.',\n",
       " 'features': 'N/A',\n",
       " 'stratified_label': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_annots = []\n",
    "with open(lh_filepath) as f:\n",
    "    for line in f:\n",
    "        tmp = json.loads(line)\n",
    "        obj = {\n",
    "            \"id\": f\"lh_{tmp['ID']}\",\n",
    "            \"img\": None,\n",
    "            \"caption\": None,\n",
    "            \"text\": tmp['post'],\n",
    "            \"label\": tmp['class'],\n",
    "            \"implicit_class\": tmp['implicit_class'],\n",
    "            \"target\": tmp['target'],\n",
    "            \"target_category\": target_mapping[tmp['target'].lower().replace('\"', \"'\")] if tmp['target'] else None,\n",
    "            \"rationale\": tmp[\"mistral_instruct_statement\"],\n",
    "            \"features\": \"N/A\"\n",
    "        }\n",
    "        obj[\"stratified_label\"] = target2label[obj['target_category']]\n",
    "        lh_annots.append(obj)\n",
    "\n",
    "lh_annots[-690]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c687493-aa8b-47d1-b95f-1271d48ad9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Targets: 326\n"
     ]
    }
   ],
   "source": [
    "target_list = set([x['target'] for x in lh_annots])\n",
    "target_list.remove(None)\n",
    "target_list = set([x.lower() for x in target_list])\n",
    "print(\"Number of Targets:\", len(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e32f3b21-d95e-4a81-b5d1-30f5d434932e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set([x['label'] for x in lh_annots])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25c02f9-ddac-4d1c-9f89-790ddfcc15f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'res_ENG01',\n",
       " 'img': '/mnt/data1/datasets/memes/Misogynistic_MEME/images/img/combined/res_ENG01.jpg',\n",
       " 'caption': 'a man and a woman in a red convertible car',\n",
       " 'web_entities': 'Car, Misogyny, Meme, Shelby Mustang, Ford Mustang, Internet meme, Text, Driving, Automatic Transmission, Manual Transmission',\n",
       " 'text': 'The way every man feels when a woman is driving',\n",
       " 'content': 'The way every man feels when a woman is driving a man and a woman in a red convertible car',\n",
       " 'label': 1,\n",
       " 'rationale': '</s> Targeted Group: Despite the lack of explicit reference to any specific group, the meme conveys a negative perception of women drivers, and as such, implicitly targets women as a group. \\n\\nDerogatory Imagery/Language: The statement \"The way every man feels when a woman is driving\" implies that women drivers cause negative feelings in men, creating an unnecessary and derogatory association between gender and driving ability.\\n\\nImpact on Bias/Stereotypes: The meme reinforces the harmful stereotype that women possess inferior driving skills, making it an example of Misogyny. Such portrayals have a negative impact on how women are perceived in society, and perpetuate biases and discrimination. \\n\\nIn summary, this meme is hateful because it targets the entire female population, and through its use of negative stereotypes and derogatory imagery, it perpetuates gender-based biases and discrimination.</s>',\n",
       " 'content_for_retrieval': 'The way every man feels when a woman is driving a man and a woman in a red convertible car',\n",
       " 'target': 'misogyny',\n",
       " 'target_category': 'sex'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misogynistic_annots = []\n",
    "with open(misogynistic_filepath) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        tmp = json.loads(line)\n",
    "        tmp['content_for_retrieval'] = tmp['content']\n",
    "        tmp['target'] = \"misogyny\"\n",
    "        tmp[\"target_category\"] = \"sex\"\n",
    "        misogynistic_annots.append(tmp)\n",
    "\n",
    "misogynistic_annots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71762d01-599e-42e1-9d5f-886f0c8aa20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set([x['label'] for x in misogynistic_annots])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd92430e-4244-44f7-8eb5-942f74e8e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in meme_templates.items():\n",
    "#     print(k, v[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209fffa6-45e1-4192-8046-761b8b9d12d9",
   "metadata": {},
   "source": [
    "### 1 - Rationale Similarity Matching\n",
    "\n",
    "Use TF-IDF to find tweets that are of similar explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e855e43-4884-478a-a94b-84976f019db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annots: 10800\n",
      "Corpus: 10800\n",
      "Labels: 10800\n"
     ]
    }
   ],
   "source": [
    "annots = lh_annots + misogynistic_annots\n",
    "corpus, labels = [], []\n",
    "for a in annots:\n",
    "    corpus.append(a['rationale'].strip())\n",
    "    labels.append(1 if a['label'] >= 1 else 0)\n",
    "\n",
    "print(\"Annots:\", len(annots))\n",
    "print(\"Corpus:\", len(corpus))\n",
    "print(\"Labels:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78cea827-d008-465e-b414-0baaf8ec506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matching.tfidf_wrapper import compute_corpus_matrix # , get_top_k_similar, get_top_k_dissimilar\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58e0b66e-7185-4df0-93e5-ac38072724c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Matrix: (10800, 15962)\n"
     ]
    }
   ],
   "source": [
    "corpus_matrix, vectorizer = compute_corpus_matrix(corpus)\n",
    "print(\"Corpus Matrix:\", corpus_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f8a91c1-346b-4d92-8e23-f077c8c6f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrices: (10800, 10800)\n"
     ]
    }
   ],
   "source": [
    "sim_matrix = cosine_similarity(corpus_matrix, corpus_matrix)\n",
    "print(\"Similarity Matrices:\", sim_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6686d9-8f2b-4665-b4a4-09ba47431c43",
   "metadata": {},
   "source": [
    "### 2 - Stage 1 Training (Tweets Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95850c91-8ca0-4472-9915-1f3b11321ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(lh_annots)\n",
    "X = df\n",
    "y = df['stratified_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c47af3b4-cf88-4139-8bb5-eaa90a059420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "stratified_label\n",
      "0    5271\n",
      "5    1117\n",
      "7     952\n",
      "2     313\n",
      "1     190\n",
      "8      69\n",
      "4      50\n",
      "6      36\n",
      "3       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Set:\n",
      "stratified_label\n",
      "0    1318\n",
      "5     280\n",
      "7     238\n",
      "2      78\n",
      "1      47\n",
      "8      17\n",
      "4      12\n",
      "6       9\n",
      "3       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Stratified Split Records for Train/Dev\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "\n",
    "# Output the split datasets\n",
    "print(\"Train Set:\")\n",
    "\n",
    "print(strat_train_set['stratified_label'].value_counts())\n",
    "print(\"\\nTest Set:\")\n",
    "print(strat_test_set['stratified_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "958e75e9-2a90-46d1-9711-d1b1085665c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'lh_803579998141452288',\n",
       " 'img': None,\n",
       " 'caption': None,\n",
       " 'text': 'hacking is a big danger  computers are not full proof and e-payment is complete hoax even e-mails are not safe .',\n",
       " 'label': 0,\n",
       " 'implicit_class': None,\n",
       " 'target': None,\n",
       " 'target_category': None,\n",
       " 'rationale': \"1. Targeted Group: The post doesn't target any specific group of people. It raises a general concern about security risks, particularly in relation to digital technology and online transactions.\\n\\n2. Derogatory Imagery or Language: The post contains no explicit derogatory imagery or language towards any particular group or individual.\\n\\n3. Impact on Bias, Stereotypes, Prejudice, Discrimination, or Inciting Harm: The post doesn't contribute to perpetuating bias, stereotypes, or inciting harm against any specific group. Instead, it expresses a security concern that affects everyone who uses digital technology and engages in online transactions.\\n\\nIn summary, the post is not hateful.\",\n",
       " 'features': 'N/A',\n",
       " 'stratified_label': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_annots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8c8c92c-1f15-4004-9888-042ce98954ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strong_positives(record, sim_vector, annots, k):\n",
    "    positives = []\n",
    "    indices = sim_vector.argsort()[::-1] # High confidence first\n",
    "    for ind in indices:\n",
    "        ind = int(ind)\n",
    "        confid = round(sim_vector[ind], 5)\n",
    "        annot = annots[ind]\n",
    "\n",
    "        obj = {\n",
    "            \"id\": annot[\"id\"],\n",
    "            \"content\": annot[\"text\"],\n",
    "            \"rationale\": annot[\"rationale\"],\n",
    "            \"confid\": confid\n",
    "        }\n",
    "\n",
    "        if record['target_category']:\n",
    "            if record['target_category'] == annot['target_category']:\n",
    "                positives.append(obj)\n",
    "        else:\n",
    "            if record['label'] == annot['label']:\n",
    "                positives.append(obj)\n",
    "\n",
    "        if len(positives) == k:\n",
    "            break\n",
    "            \n",
    "    return {\"positives\": positives}\n",
    "\n",
    "def get_strong_negatives(record, sim_vector, annots, k):\n",
    "    negatives = []\n",
    "    indices = sim_vector.argsort()[::] # Low confidence first\n",
    "    for ind in indices:\n",
    "        ind = int(ind)\n",
    "        confid = round(sim_vector[ind], 5)\n",
    "        annot = annots[ind]\n",
    "\n",
    "        obj = {\n",
    "            \"id\": annot[\"id\"],\n",
    "            \"content\": annot[\"text\"],\n",
    "            \"rationale\": annot[\"rationale\"],\n",
    "            \"confid\": confid\n",
    "        }\n",
    "\n",
    "\n",
    "        if record['target_category']:\n",
    "            if record['target_category'] != annot['target_category']:\n",
    "                negatives.append(obj)\n",
    "        else:\n",
    "            if record['label'] != annot['label']:\n",
    "                negatives.append(obj)\n",
    "\n",
    "        if len(negatives) == k:\n",
    "            break\n",
    "    return {\"negatives\": negatives}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24da836b-d665-410c-ac03-200b1942c1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:13, 727.00it/s]\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Locating records with mismatched extracted rationales\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for k in [1]:\n",
    "    training_records = []\n",
    "    for idx, record in tqdm.tqdm(enumerate(lh_annots)):\n",
    "        sim_vector = sim_matrix[idx]\n",
    "        sim_vector = np.concatenate((sim_vector[0:idx], sim_vector[idx + 1:10000]))\n",
    "        sim_annots = lh_annots[0:idx] + lh_annots[idx + 1:10000]\n",
    "        \n",
    "        pos = get_strong_positives(record, sim_vector, sim_annots, k)\n",
    "        negs = get_strong_negatives(record, sim_vector, sim_annots, k)\n",
    "        obj = {\n",
    "            \"id\": record[\"id\"],\n",
    "            \"query\": record[\"text\"],\n",
    "            \"label\": record[\"label\"],\n",
    "            \"target_category\": record[\"target_category\"],\n",
    "        }\n",
    "        obj = {**obj, **pos, **negs}\n",
    "\n",
    "        \n",
    "        key = record['id']\n",
    "        training_records.append(obj)\n",
    "    \n",
    "    print(len(training_records))\n",
    "    print(json.dumps(record, indent=2))\n",
    "    print(json.dumps(training_records, indent=2))\n",
    "    \n",
    "    with open(f\"../data/lh_training_{k}_demonstrations.json\", \"w+\") as f:\n",
    "        json.dump(training_records, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca2d5be8-65e9-4e57-af77-149ba8f5f941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 625.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{\n",
      "  \"id\": \"lh_803579998141452288\",\n",
      "  \"img\": null,\n",
      "  \"caption\": null,\n",
      "  \"text\": \"hacking is a big danger  computers are not full proof and e-payment is complete hoax even e-mails are not safe .\",\n",
      "  \"label\": 0,\n",
      "  \"implicit_class\": null,\n",
      "  \"target\": null,\n",
      "  \"target_category\": null,\n",
      "  \"rationale\": \"1. Targeted Group: The post doesn't target any specific group of people. It raises a general concern about security risks, particularly in relation to digital technology and online transactions.\\n\\n2. Derogatory Imagery or Language: The post contains no explicit derogatory imagery or language towards any particular group or individual.\\n\\n3. Impact on Bias, Stereotypes, Prejudice, Discrimination, or Inciting Harm: The post doesn't contribute to perpetuating bias, stereotypes, or inciting harm against any specific group. Instead, it expresses a security concern that affects everyone who uses digital technology and engages in online transactions.\\n\\nIn summary, the post is not hateful.\",\n",
      "  \"features\": \"N/A\"\n",
      "}\n",
      "[\n",
      "  {\n",
      "    \"id\": \"lh_803579998141452288\",\n",
      "    \"query\": \"hacking is a big danger  computers are not full proof and e-payment is complete hoax even e-mails are not safe .\",\n",
      "    \"label\": 0,\n",
      "    \"target_category\": null,\n",
      "    \"positives\": [\n",
      "      {\n",
      "        \"id\": \"lh_867725691688808453\",\n",
      "        \"content\": \"we have to be willing to take action that puts american security - - not sensitivity - - first . #terrorism\",\n",
      "        \"rationale\": \"1. Targeted Group: The post does not directly target any specific ethnic or religious group. It focuses on the broader issue of American security and the need to prioritize it over sensitivity.\\n\\n2. Derogatory Imagery or Language: The post does not contain any derogatory terms or hate speech against any individual or group. The hashtag #terrorism is neutral and refers to the act of using violence against civilians to further a political or religious agenda, without mentioning any specific religious or ethnic group associated with terrorism.\\n\\n3. Impact on Bias/Stereotypes: The post expresses a concern for American security, which is a shared value among most Americans, rather than promoting bias, stereotypes, prejudice, discrimination, or inciting harm against any specific group. It does not single out any religious or ethnic group for blame or reprisals, nor does it contribute to the stigmatization or marginalization of any particular community.\\n\\nIn summary, the post is not hateful as it does not target any specific group, contain derogatory language, or incite harm or prejudice. The focus is on promoting security for all Americans, which is an important goal in any society.\",\n",
      "        \"confid\": 0.3053\n",
      "      }\n",
      "    ],\n",
      "    \"negatives\": [\n",
      "      {\n",
      "        \"id\": \"lh_717113465517674497\",\n",
      "        \"content\": \"these people should find it hateful and offensive to behead & honor kill . not you exposing those who behead & honor kill .\",\n",
      "        \"rationale\": \"hateful.\",\n",
      "        \"confid\": 0.02709\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/lh_training_1_demonstrations.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(record, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(training_records, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../datasets/lh_training_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_demonstrations.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     30\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(training_records, f)    \n",
      "File \u001b[0;32m~/anaconda3/envs/llama-3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/lh_training_1_demonstrations.json'"
     ]
    }
   ],
   "source": [
    "# Locating records with mismatched extracted rationales\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for k in [1]:\n",
    "    training_records = []\n",
    "    for idx, record in tqdm.tqdm(enumerate(lh_annots[:1])):\n",
    "        sim_vector = sim_matrix[idx]\n",
    "        sim_vector = np.concatenate((sim_vector[0:idx], sim_vector[idx + 1:10000]))\n",
    "        sim_annots = lh_annots[0:idx] + lh_annots[idx + 1:10000]\n",
    "        \n",
    "        pos = get_strong_positives(record, sim_vector, sim_annots, k)\n",
    "        negs = get_strong_negatives(record, sim_vector, sim_annots, k)\n",
    "        obj = {\n",
    "            \"id\": record[\"id\"],\n",
    "            \"query\": record[\"text\"],\n",
    "            \"label\": record[\"label\"],\n",
    "            \"target_category\": record[\"target_category\"],\n",
    "        }\n",
    "        obj = {**obj, **pos, **negs}\n",
    "    \n",
    "        key = record['id']\n",
    "        training_records.append(obj)\n",
    "    \n",
    "    print(len(training_records))\n",
    "    print(json.dumps(record, indent=2))\n",
    "    print(json.dumps(training_records, indent=2))\n",
    "    \n",
    "    with open(f\"../datasets/lh_training_{k}_demonstrations.json\", \"w+\") as f:\n",
    "        json.dump(training_records, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c0ae3-691a-4e5e-aef2-11c72b4bf43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287c975-d37a-4426-bb73-c8d13e6c9edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee60dc21-6f70-4347-a207-234a7b2f91d4",
   "metadata": {},
   "source": [
    "### Data for Facebook's GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7dc882-335d-44e3-9b1b-4b8f2a41cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strong_positives(record, sim_vector, annots, k):\n",
    "    positives = []\n",
    "    indices = sim_vector.argsort()[::-1] # High confidence first\n",
    "    for ind in indices:\n",
    "        ind = int(ind)\n",
    "        confid = round(sim_vector[ind], 5)\n",
    "        annot = annots[ind]\n",
    "\n",
    "        obj = {\n",
    "            \"text\": record[\"rationale\"]\n",
    "        }\n",
    "\n",
    "        if record['target_category']:\n",
    "            if record['target_category'] == annot['target_category']:\n",
    "                positives.append(obj)\n",
    "        else:\n",
    "            if record['label'] == annot['label']:\n",
    "                positives.append(obj)\n",
    "\n",
    "        if len(positives) == k:\n",
    "            break\n",
    "            \n",
    "    return {\"positive_ctxs\": positives}\n",
    "\n",
    "def get_strong_negatives(record, sim_vector, annots, k):\n",
    "    negatives = []\n",
    "    indices = sim_vector.argsort()[::] # Low confidence first\n",
    "    for ind in indices:\n",
    "        ind = int(ind)\n",
    "        confid = round(sim_vector[ind], 5)\n",
    "        annot = annots[ind]\n",
    "\n",
    "        obj = {\n",
    "            \"passage\": record[\"rationale\"]\n",
    "        }\n",
    "\n",
    "\n",
    "        if record['target_category']:\n",
    "            if record['target_category'] != annot['target_category']:\n",
    "                negatives.append(obj)\n",
    "        else:\n",
    "            if record['label'] != annot['label']:\n",
    "                negatives.append(obj)\n",
    "\n",
    "        if len(negatives) == k:\n",
    "            break\n",
    "    return {\"negative_ctxs\": negatives}\n",
    "\n",
    "# Locating records with mismatched extracted rationales\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for k in [1]:\n",
    "    training_records = []\n",
    "    for idx, record in tqdm.tqdm(enumerate(lh_annots[:1])):\n",
    "        sim_vector = sim_matrix[idx]\n",
    "        sim_vector = np.concatenate((sim_vector[0:idx], sim_vector[idx + 1:10000]))\n",
    "        sim_annots = lh_annots[0:idx] + lh_annots[idx + 1:10000]\n",
    "        \n",
    "        pos = get_strong_positives(record, sim_vector, sim_annots, k)\n",
    "        negs = get_strong_negatives(record, sim_vector, sim_annots, k)\n",
    "        obj = {\n",
    "            \"id\": record[\"id\"],\n",
    "            \"query\": record[\"text\"],\n",
    "            \"label\": record[\"label\"],\n",
    "            \"target_category\": record[\"target_category\"],\n",
    "        }\n",
    "        obj = {**obj, **pos, **negs}\n",
    "    \n",
    "        key = record['id']\n",
    "        training_records.append(obj)\n",
    "    \n",
    "    print(len(training_records))\n",
    "    print(json.dumps(record, indent=2))\n",
    "    print(json.dumps(training_records, indent=2))\n",
    "    \n",
    "    with open(f\"../datasets/lh_training_{k}_demonstrations.json\", \"w+\") as f:\n",
    "        json.dump(training_records, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb223e-a546-4a74-b3c5-abbfec839bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471805cf-c2ca-446b-8587-21c7e05f9fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03d814-59da-4d56-a24c-39d15ebdd441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
